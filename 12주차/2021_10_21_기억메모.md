## Lecture 9 : Closed-book QA with T5

### 1. Closed-book Question Answering

- Current approaches of building QA system

  ![a](https://user-images.githubusercontent.com/87477828/138220764-f045a499-a968-43b4-8c39-f60317eb31ed.png)

- Idea of Closed-book Question Answering

  - Open domain vs Open book
    - Open domain은 질문이 주어졌을 때 답을 주는 task 자체
    - Open book/Closed book은 질문에 대한 답을 낼 때 모델에게 책이라 볼 수 있는 거대한 web corpus를 접근하도록 허용해 줄 것이냐 안 줄 것이냐를 뜻함

  - 모델이 이미 사전학습으로 대량의 지식을 학습했다면, 사전학습 언어모델 자체가 이미 하나의 knowledge storage 라고 볼 수 있지 않을까? => 굳이 다른 곳에서 지식을 가져와야할 필요가 없지 않을까?

    ![a1](https://user-images.githubusercontent.com/87477828/138220767-9c6e1175-5dd3-4a3e-8264-35dffb12d590.png)

- Zero-shot QA performance of GPT-2

  - 사전학습 시 전혀 본 적 없는 Natural Question 데이터셋에도 어느 정도 대답이 가능함

    ![a](https://user-images.githubusercontent.com/87477828/138221603-71323d56-6cbd-41d0-922a-9a00c60c689f.png)

- Open-book QA vs. Closed-book QA

  ![a1](https://user-images.githubusercontent.com/87477828/138221605-48f5e02b-1367-448c-8bbc-990b87516747.png)

### 2. Text-to-Text Format

- closed-book QA as Text-to-Text Format

  - Closed-book QA에 사용되는 방법은 Generation-based MRC와 유사함 [3강](https://github.com/SinGonKim/summary_boostcamp/blob/master/11%EC%A3%BC%EC%B0%A8/2021_10_14_(1)_%EA%B8%B0%EC%96%B5%EB%A9%94%EB%AA%A8.md) 참고

    - 단, 입력에 지문(Context)가 없이 질문만 들어간다는 것이 차이점

    - 사전학습된 언어 모델은 BART와 같은 seq-to-seq 형태의 Transformer 모델을 사용함

    - Text-to-Text format에서는 각 입력값(질문)과 출력값(답변)에 대한 설명을 추가함.

      ![a](https://user-images.githubusercontent.com/87477828/138222281-4b2204ea-0285-4f7a-a90b-3c9cc49ac9a9.png)

- Text- to - Text Format

  - Text-to-text problem: input으로 text를 받아서, output으로 새로운 text를 생성하는 문제

  - 다양한 text processing problem => Text- to - text 문제로 변형

    ![a](https://user-images.githubusercontent.com/87477828/138222886-8c0889b3-5a06-4d7b-bdd7-1546662acb3d.png)

- Text-to-Text Format Example 1

  - Task-specific prefix를 추가 => 특정 task에 알맞은 output text를 생성하도록

    EX1) Machine translation: prefix = translate A to B (A: source language/ B: target language)

    ![a1](https://user-images.githubusercontent.com/87477828/138224428-18613820-e8ef-4d0d-98e2-8f844853fffd.png)

- Text- to-Text Format Example 2

  - Ex2) Text Classification (MNLI)

    - MNLI: 두 개의 sentence (premise, hypothesis)가 주어지고, 이 둘의 관계를 예측하는 task (neutral, contradiction, entailment)

    - Input: "mnli hypothesis <sent1> premise: <sent2>"

    - output: "neutral" or "contradiction" or "entailment"

      ![a2](https://user-images.githubusercontent.com/87477828/138224431-4c27bb97-d826-423b-a5a7-47a7c21fb176.png)

- Model Overview

  - BART와 같은 모델의 상위호환

    ![a1](https://user-images.githubusercontent.com/87477828/138225067-d7200d98-dfe7-44b6-9988-62ead21042b0.png)

- T5

  - Text-to-Text format 이라는 형태로 데이터의 입출력을 만들어 거의 모든 자연어처리 문제를 해결하도록 학습된 seq-to-seq 형태의 Transformer 모델

    ![a](https://user-images.githubusercontent.com/87477828/138225285-cb96a99e-aea0-480e-bc5f-c59c0fffc479.png)

- Pre-training T5

  - 다양한 모델 구조, 사전학습 목표, 사전학습용 데이터, Fine-tuning 방법 등을 체계적으로 실험함

  - 가장 성능이 좋은 방식들을 선택하여 방대한 규모의 모델을 학습시킴

    ![a](https://user-images.githubusercontent.com/87477828/138225787-a62a4c3c-c2b9-4b51-81e9-9ebe65955788.png)

    ​								![a1](https://user-images.githubusercontent.com/87477828/138225790-2a93513f-7931-4c35-b748-78eae7dbc263.png)

- Using T5 for Closed-book QA

  - Fine-tuning T5:

    - 미리 학습된 pre-trained T5를 활용

    - Fine-tuning: MRC 데이터셋 (TriviaQA, WebQuestions, Natural Questions)의 QA  pair를 활용

      ![a1](https://user-images.githubusercontent.com/87477828/138226532-ccf445be-ed56-4301-a12a-e0abb0ccb92f.png)

    - MRC 데이터셋에 제공되는 supporting document는 무시

    - Input : Task-specific prefix 추가 => "trivia question: <question>"

    - Natural Questions와 같이 답이 여러개인 경우 => "answer: <answer 1> answer: <answer2>"

      ![a2](https://user-images.githubusercontent.com/87477828/138226537-370e2cb6-d69b-4d11-80f6-2ffa7a310aaf.png)

### 3. Experiment Results & Analysis

- Experiment Setting
  - Dataset
    - Open-domain QA 데이터셋 또는 MRC 데이터셋에서 지문을 제거하고 질문과 답변만 남긴 데이터셋을 활용
  - Salient Span Masking
    - 고유 명사, 날짜 등 의미를 갖는 단위에 속하는 토큰 범위를 마스킹한 뒤 학습
    - Pre-traned 체크포인트에서 추가로 pre-training 함
  - Fine-tuning
    - Pre-trained T5 체크포인트를 Open-domain QA 학습 데이터셋으로 추가 학습

- Experiment Examples

  - T5를 이용한 Closed-book Question Answering 예시

    ![a](https://user-images.githubusercontent.com/87477828/138227648-41665792-8534-45c4-b364-5870c59918e6.png)

- Quantitative Examples
  - 대부분의 Open-book 스타일 모델 (문서 검색 후 기계 독해) 보다 뛰어난 성능을 보여줌
  - 모델 크기가 커질수록 성능이 증가함
  - Sailent Span Masking이 성능을 크게 끌어올림

- Prediction Examples

  - T5를 이용한 Clsoed-book Question Answering 예시

    ![a](https://user-images.githubusercontent.com/87477828/138228132-fbd95979-2f46-4822-b33b-6887cfef6b87.png)

     - 오답도 나오지만 답이 완전히 틀리지는 않음

- False negatives

  - Exact match 기준으로 오답으로 채점된 결과를 사람이 평가한 결과 오답이 아닌 경우

    - Phrasing Mismatch: 정답에 대한 표현이 다른 경우

    - Incomplete Annotation: 정답이 여러 개일 수 있으나 하나만 정답으로 처리되는 경우

    - Unanswerable: 질문을 한 시간이나 문맥에 따라서 정답이 달라지지 않음

      ![a](https://user-images.githubusercontent.com/87477828/138268523-bee872da-4388-45b4-897e-189d484d86a6.png)

- LImitations

  - Closed - book QA의 한계점 및 앞으로의 개선 방향

    - 모델의 크기가 커서 계산량이 많고 속도가 느림
      - 더 효율적인 모델 필요

    - 모델이 어떤 데이터로 답을 내는지 알 수 없음
      - 결과의 해석 가능성(interpretability)을 높이는 연구

    - 모델이 참조하는 지식을 추가하거나 제거하기 어려움

