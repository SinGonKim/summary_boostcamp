## Lecture 8 : Reducing Training Bias

### 1. Definition of Bias

- Bias 종류
  - Bias in learning
    - 학습할 때 과적합을 막거나 사전 지식을 주입하기 위해 특정 형태의 함수를 선호한다.(inductive bias)
    - Bias가 있다고 무조건 나쁜 것도 아니고 좋은 것도 아니다.
  - A Biased World
    - 현실 세계가 편향되어 있기 때문에 모델에 원치 않는 속성이 학습되는 것(historical bias)
    - 성별과 직업 간 관계 등 표면적인 상관관계 때문에 웒지 않는 속성이 학습되는 것 (co-occurence bias)
  - Bias in Data Generation
    - 입력과 출력을 정의한 방식 때문에 생기는 편향(specific bias)
    - 데이터를 샘플링한 방식 때문에 생기는 편향(sampling bias)
    - annotator features 때문에 생기는 편향 (annotator bias)

- Gender Bias
  - 대표적인 bias 예시
  - 특정 성별과 행동을 연관시켜서 예측 오류가 발생

- Sampling Bias
  - 1936년 <리터러시 다이제스트> 대통령 선거 여론조사
    - 표본크기 : 240만 명 (사상 최대)
    - 예측 : 루즈벨트 43%, 알프레드 랜던 57% -> 실제 : 루즈벨트 62%, 알프레드 랜던 38%
    - 설문 대상 : 잡지 정기구독자, 자동차 등록명부, 사교클럽 인명부 등 -> 중산층 이상으로 표본이 왜곡됨(편향된 대상)

### 2. Bias in Open-domain Question Answering

- Retriever - Reader Pipeline

  ![a](https://user-images.githubusercontent.com/87477828/138213981-cdef26ce-33e3-425b-98ce-42aa821cedd8.png)

- Training bias in reader model

  - 만약 reader 모델이 만약 한정된 데이터셋에서만 학습이 된다면...

    - Reader은 항상 정답이 문서 내에 포함된 데이터쌍만(Positive)을 보게 됨

    - 특히 SQuAD 와 같은 (context, Query, Answer)이 모두 포함된 데이터는 positive가 완전히 고정되어 있음

      - Inference 시 만약 데이터 내에서 찾아 볼 수 없었던 새로운 문서를 준다면?

      - Reader 모델은 문서에 대한 독해 능력이 매우 떨어질 것이고, 결과적으로 정답을 내지 못할 것임

        ![a1](https://user-images.githubusercontent.com/87477828/138213985-5953fb01-a251-4ed6-8679-7b0bea20d156.png)

- How to mitigate training bias?
  - Train negative examples
    - 훈련할 때 잘못된 예시를 보여줘야 retriever이 negative한 내용들을 먼 곳에 배치할 수 있다.
      - negative sample도 완전히 다른 negative sample과 비슷한 negative sample에 대한 차이 고려가 필요함
    - 어떻게 [좋은 negative sample](https://github.com/SinGonKim/summary_boostcamp/blob/master/11%EC%A3%BC%EC%B0%A8/2021_10_15_(1)_%EA%B8%B0%EC%96%B5%EB%A9%94%EB%AA%A8.md)을 만들 수 있을까? (5강)
      - Corpus 내에서 랜덤하게 뽑기
      - 좀 더 헷갈리는 negative samples 뽑기
        - 높은 BM25/ TF-IDF 매칭 스코어를 가지지만 , 답을 포함하지 않는 샘플
        - 같은 문서에서 나온 다른 Passage/Question 선택하기
  - Add no answer bias
    - 입력 시퀀스의 길이가 N일 때, 시퀀스의 길이 외 1개의 토큰이 더 있다고 생각하기
      - 훈련 모델에 마지막 레이어 weight에 훈련 가능한 bias를 하나 더 추가
      - softmax로 answer prediction을 최종적으로 수행할 때, start end 확률이 해당 bias 위치에 있는 경우가 가장확률이 높으면 이는 "대답할 수 없다"라고 취급



### 3. Annotation Bias from Datasets

- What is annotation bias?

  - Annotation bias : ODQA 학습 시 기존의 MRC 데이터셋 활용

    - ODQA 세팅에는 적합하지 않은 bias가 데이터 제작(annotation) 단계에서 발생할 수 있음

      ![a](https://user-images.githubusercontent.com/87477828/138215266-8a3df9d4-b98d-4835-9bf8-6e84f0a570bd.png)
      ![a1](https://user-images.githubusercontent.com/87477828/138215268-57d0e9fa-d3a1-47cd-b610-22b70057f8ab.png)
      ![a3](https://user-images.githubusercontent.com/87477828/138215273-6b2a35ff-0831-4495-b604-fd502afc8091.png)

- Effect of annotation bias

  ![a](https://user-images.githubusercontent.com/87477828/138215612-73c733bd-70c2-4cb0-b868-5c996396d580.png)

  ​					(BM25 : Sparse embedding / DPR : dense embedding)

- Dealing with annotation bias

  - Annotation 단계에서 발생할 수 있는 bias를 인지하고, 이를 고려하여 데이터를 모아야함

    ex) ODQA 세팅과 유사한 데이터 수집 방법

    - Natural Questions: Supporting evidence가 주어지지 않은, 실제 유저의 Question들을 모아서 dataset을 구성

      - [예시](https://ai.google.com/resarch/NaturalQuestions/visualization)

        ![a](https://user-images.githubusercontent.com/87477828/138215986-ff8355f4-6455-4dca-8b01-f336ee07531d.png)

- Another bias in MRC dataset

  - SQuAD : Passage가 주어지고, 주어진 passage 내에서 질문과 답을 생성

    - ODQA에 application하지 않은 질문들이 존재

      - ex) 미국 대통령이 누구인가? (현재인지, 과거인지...)

        ![a](https://user-images.githubusercontent.com/87477828/138216167-4e010193-2c83-4d92-8a59-1e7a95c5f078.png)

