## Lecture 10 : QA with Phrase Retrieval

### 1. Phrase Retrieval in Open-Domain Question Answering

- Current limitation of Retriever-Reader approach

  - Error Propagation: 5-10개의 문서만 reader에게 전달됨

  - Query-dependent encoding: query에 따라 정답이 되는 answer span에 대한 encoding이 달라짐

    ![a](https://user-images.githubusercontent.com/87477828/138395031-2e265541-692f-47a4-b461-84d55f62c9e5.png)

- How does Document Search work?

  ![a1](https://user-images.githubusercontent.com/87477828/138395034-50edb7e2-b0d3-4a80-9512-797853c93ef6.png)

- One solution: Phrase Indexing

  ![a](https://user-images.githubusercontent.com/87477828/138395208-221f67fc-1beb-45f5-8473-44445df54d4c.png)

- Query-Agnostic Decomposition

  ![a](https://user-images.githubusercontent.com/87477828/138395407-6797dc00-4025-4971-8d44-8d78923cf666.png)

  ​									항상 Question encoder과 Phrase encoder로 나눌 수 있을까?

  ​									- 나눌 수 있다면 매우 효율적임

  ​		![a1](https://user-images.githubusercontent.com/87477828/138395410-1b7fc04f-7d75-4f70-a883-3582ba9289f5.png)

     - 두개를 합쳐보자!

       

### 2. Dense-sparse Representation for Phrases

- Dense vectors vs Sparse vectors

  - Dense vectors: 통상적, 의미적 정보를 담는데 효과적

  - Sparse vectors: 어휘적 정보를 담는데 효과적

    ![a](https://user-images.githubusercontent.com/87477828/138396083-e7d2776f-55fa-471d-9fff-80006a276a30.png)

- Phrase and Question Embedding

  - Dense vector와 sparse vector를 모두 사용하여 phrase (and question) embedding

    ![a](https://user-images.githubusercontent.com/87477828/138396411-8f835fa2-6fa2-4ff8-97d0-547cbe3db119.png)

- Dense representation

  - Dense vector를 만드는 방법

    - Pre-trained LM (e.g BERT)이용

    - Start vector와 end vector를 재사용하여 메모리 사용량 줄임

      ![a1](https://user-images.githubusercontent.com/87477828/138396705-fd5cb4a1-2dd3-4583-8495-c2ab73ce5584.png)

  - Coherency representation

    - Phrase가 한 단위의 문장 구성 요소에 해당하는지 나타냄

    - 구(句)를 형성하지 않는 phrase를 걸러내기 위해 사용함

    - Start vector와 end vector를 이용하여 계산

      ![a2](https://user-images.githubusercontent.com/87477828/138396708-11d38755-0ad8-40d3-a3f1-402c9b754ec4.png)

  - Question embedding

    - Question을 임베딩할 때는 [CLS] 토큰 (BERT)활용

      ​	![a3](https://user-images.githubusercontent.com/87477828/138396711-ba895f3f-6cf0-4b2e-8c30-8756ef736449.png)

- Sparse representation

  - Sparse vector를 만든느 방법

    - 문맥화된 임베딩(contextualized embedding)을 활용하여 가장 관련성이 높은 n-gram으로 sparse vector 구성

      ![a4](https://user-images.githubusercontent.com/87477828/138396712-b30ef714-d6ef-4694-ae0d-c6ac909d95a7.png)

- Scalability Challenge

  - In Wikipedia: 60 billion개의 phrases가 존재 => Storage, indexing, search의 scalability가 고려되어야함

    - Storage: Pointer, filter, scalar quantization 활용(240T storage => 1.4T storage)

    - Search: FAISS를 활용해 dense vector에 대한 search를 먼저 수행 후 sparse vector로 reranking

      ![a](https://user-images.githubusercontent.com/87477828/138397622-c45906da-bf61-4b63-a565-b20630126fd2.png)

### 3. Experiment Results & Analysis

- Experiment Results -SQuAD - open
  - SQuAD-open (Open-domain QA)
    - s/Q: seconds per query on CPU
    - #D/Q: number of documents visited per query
    - DrQA (Retriever-reader)보다 +3.6% 성능/ 68x 빠른 inference spped(less than 1s)

- Experiment Results

  ![a](https://user-images.githubusercontent.com/87477828/138398627-b6b1043a-ac92-4ed8-a321-bcbf27a4f183.png)

- Limitation in Phrase Retrieval Approach

  - Large storage required: 2TB SSD for phrases
  - 최신 Retriever-reader 모델들 대비 낮은 성능

  - Decomposability gap
    - 기존 Question, passage, answer가 모두 함께 encoding
      - (phrase retrieval) Question과 passage/answer이 각각 encoding
        - Question과 passage사이 attention 정보x

