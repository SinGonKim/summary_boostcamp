## 기록의 양분화

앞으로 git hub에는 생각의 구현, code에 신경을 쓰고 T-story에는 복습의 내용에 대해서 다루는 것으로 양분화 할려고 한다.

[2021_08_09 기록](https://thought-process-ing.tistory.com/2)



## MNIST를 이용해 MLP설계

필수과제1과 연관되어있는 내용이므로 코드를 전체를 쓰지는 못할 것 같으므로 사고를 위주로 기록을 하겠다.

또한 검증과정은 과감히 머리속으로만 생각하도록 하고 초기 설계까지만 적도록 하겠다.

```python
#1 Module import
import numpy as np #선형대수와 관련된 함수를 쉽게 이용할 수 있는 모듈
import matplotlib.pyplot as plt #함수의 결과물에 대한 수치를 쉽게 이해할 수 있도록 시각화 할 수 있는 외부모듈
import torch #파이토치 기본모듈
import torch.nn as nn #딥러닝 설계할 때 필요한 함수를 모아 놓은 모듈
import torch.nn.functional as F #torch.nn 중에서도 자주 이용되는 함수를 'F'로 지정
from torchvision import transforms, datasets #컴퓨터 비전 연구 분야에서 자주 이용하는 'torchvision'모듈 내 'transforms', 'datasets'함수를 임포트
```

```python
#2 딥러닝 모델을 설계할 때 활용하는 장비 확인
%config InlineBackend.figure_format='retina' #더 잘보인다.
print ("PyTorch version:[%s]."%(torch.__version__)) #장치
if torch.cuda is_available(): #gpu쓸 수 있으면 gpu 없으면 cpu써라.
    DEVICE = torch.device('cuda')
else:
    DEVICE = torch.device('cpu')
print ("device:[%s]."%(device))
```

```python
#3 MNIST 데이터 다운로드
train_dataset = datasets.MNIST(root='../data/MNIST',train=True, transform = transforms.ToTensor(), download=True)
test_dataset = datasets.MNIST(root='../data/MNIST', train=False, transform=transforms.ToTensor(), download=True)
```

```python
#4 Batch 단위로 분리해 지정하여 mini-batch를 구성하는 것을 'DataLoader' 함수를 이용해 지정
BATCH_SIZE=32
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE,shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset= test_dataset, batch_size=BATCH_SIZE, shuffle= False)

```

```python
#5 MLP모델 설계하기
class MultiLayerPerceptronClass(nn.Module):#클래스 정의
    def __init__(self):#MultiLayerPerceptronClass 클래스의 인스턴스를 생성했을 때 지니게 되는 성질을 정의
        super(MultiLayerPerceptronClass,self).__init__() #nn.Module 내에 있는 메서드를 상속받아 이용
        self.func_1 = nn.Linear(self,a,b) #input a에서 output b로 이동
        self.func_2 = nn.Lineaer(self,a,b,) #input b에서 output c로 이동
    def forward(self,x):
        x=x.view(-1,a) #초기 입력값
        x = self.func_1(x)
        x = F.relu(net) #activation 활성화 함수 사용
        x = self.func_2(x)
        return x
    
```

```python
#6 optimizer, objective function 설정
Model = MultiLayerPerceptronClass().to(device)
optimizer = torch.optim.SGD(Model.parameters(), lr=0.001, momentum= 0.1) #다른 함수 쓸 때도 많음 ex) Adam. sigmoid...
```

