## Lecture 4 : Passage Retrieval- Sparse Embedding

### 1. Introduction to Passage Retrieval

- Passage Retrieval

  - 질문(Query)에 맞는 문서(Passage)를 찾는 것

    ![1](https://user-images.githubusercontent.com/87477828/137256857-6037c8a1-1e48-4b88-95a8-8a68290791d7.png)

    - Passage Retrieval 목표는 위키피디아, www를 통해 질문에 맞는 답변을 가져오는 것이다.

      ![2](https://user-images.githubusercontent.com/87477828/137256996-506e792d-ada2-4462-a55f-d85073c0a13e.png)

- Passage Retrieval with MRC

  - Open-domain Question Answering : 대규모의 문서 중에서 질문에 대한 답을 찾기

    - Passage Retrieval과 MRC를 이어서 2-Stage로 만들 수 있음.

      ![10](https://user-images.githubusercontent.com/87477828/137257145-59c7eb8c-16be-4bd0-a29e-905d72d99941.png)

      - Passage Retrieval : 질문과 관련된 문서찾기 or 답을 포함하는 문서찾기
      - MRC : 질문에 대한 답을 문서에서 찾기

- Overview of Passage Retrieval

  - Query(질문)와 Passage(문서)를 임베딩한 뒤 유사도로 랭킹을 매기고, 유사도가 가장 높은 Passage를 선택함

    ![1](https://user-images.githubusercontent.com/87477828/137257561-62e9f08c-d28b-45f4-a328-ef3ec8964e1f.png)

## 2. Passage Embedding and Sparse Embedding

- Passage Embedding Space

  - Passage Embedding의 벡터 공간.

  - 벡터화된 Passage를 이용하여 Passage 간 유사도 등을 알고리즘으로 계산할 수 있음.

    ![1](https://user-images.githubusercontent.com/87477828/137257978-4ac5ae3a-4c18-44fe-959a-e848b974fc7f.png)

- Sparse Embedding 소개

  - Sparse는 Dense의 반대어이다.

    ![1](https://user-images.githubusercontent.com/87477828/137258151-2ed919c3-f3ec-4d79-89bc-7afe6ec424b4.png)

    ​		- 단어가 Doc에 있으면 1 없으면 0

    - BOW를 구성하는 방법 -> n-gram
      - uni-gram (1-gram) : It was the best of times => It, was, the , best, of, times
      - bi-gram (2-gram) : It was the best of tiems => It was ,was the, the best, best of, of times
      - 데이터가 기하급수적으로 증가하기 때문에 한다면 대부분 bi-gram까지 사용

    - Term Value를 결정하는 방법
      - Term이 document에 등장하는지 (binary)
      - Term 이 몇 번 등장하는지 (term frequency), 등. (e.g. TF-IDF)

    ![1](https://user-images.githubusercontent.com/87477828/137258530-30e5432f-719d-49e1-9543-db04ee944405.png)

- Sparse Embedding 특징

  - Demension of embedding vector = number of terms

    - 등장하는 단어가 많을수록 증가

    - N-gram의 n이 커질수록 증가

      ![1](https://user-images.githubusercontent.com/87477828/137259014-f8671fc8-bd8b-479e-99f8-c171e1db739b.png)

  - Term overlap을 정확하게 잡아 내야할 때 유용.

  - 반면, 의미(semantic)가 비슷하지만 다른 단어인 경우 비교가 불가

    ![1](https://user-images.githubusercontent.com/87477828/137259232-013b7872-c8af-40ba-926e-37ee2ca1797e.png)

    ​		- Term overlap을 정확하게 잡아냄. Sparse Embedding이 유용하게 쓰인 사례

### 3. TF-IDF

- TF - IDF (Term Frequency - Inverse Document Frequency) 소개

  - Term Frequency (TF) : 단어의 등장빈도

  - Inverse Document Frequency (IDF) : 단어가 제공하는 정보의 양

    ex) It was the best of times

    -> It, was, the, of : 자주 등장하지만 제공하는 정보량이 적음

    -> best, times : 좀 더 많은 정보를 제공 (더 많은 점수를 주는 것이 좋다.)

    ![1](https://user-images.githubusercontent.com/87477828/137259829-b5963822-3ab9-4049-b700-d13638ebe4cd.png)

- Term Frequency(TF)

  - 해당 문서 내 단어의 등장 빈도

    - Raw count

    - Adjusted for doc length: raw count/ num words (TF)

    - Other variants : binary, log normalization, etc.

      ![1](https://user-images.githubusercontent.com/87477828/137260015-7984721b-9c54-4dfa-8c80-f2f01f290781.png)

- Inverse Document Frequency (IDF)

  단어가 제공하는 정보의 양

  ```latex
  IDF(t) = log(N/DF(t))
  ```

  Document Frequency(DF) = Term t가 등장한 document의 개수

  N : 총 document의 개수

  ![1](https://user-images.githubusercontent.com/87477828/137260433-f70902ae-be8e-424b-a752-480286e3d3b8.png)

- Combine TF&IDF

  TF-IDF(t,d) : TF-IDF for term t in document d,

  ​	![1](https://user-images.githubusercontent.com/87477828/137262899-4e1e8fc6-dc58-494a-bfdb-cdf8428700ba.png)

  - 'a', 'the' 등 관사 => Low TF-IDF
    - TF는 높을 수 있지만, IDF가 0에 가깝기 때문에

  - 자주 등장하지 않는 고유 명사 (ex. 사람, 이름, 지명 등) => High TF-IDF
    - IDF가 커지면서 전체적인 TF-IDF 값이 증가

- TF-IDF 계산하기

  - 실험할 데이터

    ![1](https://user-images.githubusercontent.com/87477828/137263423-11f0a01e-4481-42db-992d-3ac3d61d011e.png)

  - Tokenizer

    ![2](https://user-images.githubusercontent.com/87477828/137263515-2622f477-052a-4654-b4d1-35b9c2ce26c9.png)

  - TF

    ![3](https://user-images.githubusercontent.com/87477828/137263427-5bc60ef1-17c6-4922-86b7-abad624d1961.png)

  - IDF

    ![4](https://user-images.githubusercontent.com/87477828/137263428-e304c7f9-b423-47d0-903f-841900a10764.png)

  - TF-IDF

    ![5](https://user-images.githubusercontent.com/87477828/137263429-47d2bd33-bcd1-4280-bec0-a9d8719c4b21.png)

- TF-IDF를 이용해 유사도 구해보기

  - 목표: 계산한 문서 TF-IDF를 가지고 질의 TF-IDF를 계산한 후 가장 관련있는 문서를 찾기

    - 질문 : 주연은 BTS의 누구를 가장 잘생겼다고 생각한다?

      ![1](https://user-images.githubusercontent.com/87477828/137263831-1ba34ff8-df92-433e-93fb-08e0e456188d.png)

  - 목표 : 계산한 TF-IDF를 가지고 사용자가 물어본 질의에 대해 가장 관련 있는 문서를 찾자

    1. 사용자가 입력한 질의를 토큰화

    2. 기존에 단어 사전에 없는 토큰들은 제외

    3. 질의를 하나의 문서로 생각하고, 이에 대한 TF-IDF 계산

    4. 질의 TF-IDF 값과 각 문서별 TF-IDF값을 곱하여 유사도 점수 계산

       ![1](https://user-images.githubusercontent.com/87477828/137264031-fa97b80e-0089-4b0c-8135-3ab6edfd6739.png)

    5. 가장 높은 점수를 가지는 문서 선택

  - 목표 : 계산한 TF-IDF를 가지고 사용자가 물어본 질의에 대해 가장 유사한 문서를 검색하기

    ![1](https://user-images.githubusercontent.com/87477828/137264285-5c74d09b-dfb4-4ba9-9c46-1769ca3f314c.png)

- BM25

  - TF-IDF의 개념을 바탕으로 문서의 길이까지 고려하여 점수를 매김

  - TF 값에 한계를 지정해두어 일정한 범위를 유지하도록 함

  - 평균적인 문서의 길이 보다 더 작은 문서에서 단어가 매칭된 경우 그 문서에 대해 가중치를 부여

  - 실제 검색엔진, 추천 시스템 등에서 아직까지도 많이 사용되는 알고리즘

    ![1](https://user-images.githubusercontent.com/87477828/137264509-7cd0abf2-7708-43ea-924c-f8499db20701.png)

    [참고자료](https://github.com/castorini/pyserini/blob/master/docs/experiments-msmarco-doc.md)

