## Lecture 5: Passage Retrieval - Dense Embedding

### 1. Introduction to Dense Embedding

- Passage Embedding

  - 구절(Passage)을 벡터로 변환하는 것

    ![a](https://user-images.githubusercontent.com/87477828/137322481-869fee83-b55b-41e8-a712-306b00437d21.png)

- Sparse Embedding

  - TF-IDF벡터는 Sparse하다.

    ![a](https://user-images.githubusercontent.com/87477828/137322761-95778d84-2afd-40ac-b879-4fd015749624.png)

    - 많은 항이 0.0이다.

- Limitations of sparse embedding

  - 차원의 수가 매우 크다. -> compressed format으로 극복 가능

  - 유사성을 고려하지 못한다. -> Sparse embedding의 가장 큰 단점

    ![a](https://user-images.githubusercontent.com/87477828/137323058-2eb88009-67ef-479c-a4f0-f3bfd986bf5c.png)

- Dense Embedding이란?

  - Complementary to sparse representations by design

    - 더 작은 차원의 고밀도 벡터 (length = 50~100)

    - 각 차원이 특정 term에 대응되지 않음(차원이 합쳐져서 vector_space의 상의 위치를 나타내도록 복합적인, 부분적인 의미를 나타냄)

    - 대부분의 요소가 non-zero 값

      ![a](https://user-images.githubusercontent.com/87477828/137323542-3f0e908a-c65c-4f28-adf7-b79b5983e53f.png)

- Retrieval : Sparse vs Dense

  ![a](https://user-images.githubusercontent.com/87477828/137325714-0bb4d8f3-441d-4579-93d4-69992a8cfa84.png)

  ​	- 최근 사전학습 모델의 등장, 검색 기술의 발전 등으로 인해 Dense Embedding을 활발히 이용

- Overview of Passage Retrieval with Dense Embedding

  ![a](https://user-images.githubusercontent.com/87477828/137326408-7b28549d-83d3-46f4-809d-7f9c8bb11ff1.png)
  ![a1](https://user-images.githubusercontent.com/87477828/137326413-34817d87-9a48-4a83-a9ac-cbd60a674a45.png)
  ![a2](https://user-images.githubusercontent.com/87477828/137326416-3b9efe16-6e2c-430e-9954-d3fc02bc188d.png)

  - 두 벡터는 같은 차원이다.
  - 가장 많이 쓰이는 유사도는 두 벡터를 내적하는 것이다.

### 2. Training Dense Encoder

- What can be Dense Encoder

  - BERT와 같은 Pre-trained language model(PLM)이 자주 사용

  - 그 외 다양한 neural network 구조도 가능

    ![a](https://user-images.githubusercontent.com/87477828/137328868-ba1ac06d-1e09-432b-be14-0b7d856609bb.png)

  - BERT as dense encoder -> [CLS] token의 output 사용

  ![a](https://user-images.githubusercontent.com/87477828/137329900-8406dd25-29f9-49e7-ba7f-77a6fe95a295.png)

- Dense Encoder 구조

  ![a](https://user-images.githubusercontent.com/87477828/137330911-a3333661-b549-4fe4-b10e-15129475404b.png)

  ​			- 유사도가 크면 값이 최대한 커지도록 하고 유사도의 크기가 작으면 최대한 마이너스쪽으로 가도록 함

- Dense Encoder 학습 목표와 학습 데이터

  - 학습 목표 : 연관된 question과 passage dense embedding 간의 거리를 좁히는 것 (또는 inner product를 높이는 것), 즉 higher similarity.

  - Challenge : 연관된 question/passage를 어떻게 찾을 것인가?

    - 기존 MRC 데이터셋을 활용

      ![a](https://user-images.githubusercontent.com/87477828/137331846-87596872-1af5-4539-b438-ae69b8fe7699.png)

- Dense Encoder 학습 목표와 학습 데이터 - Negative Sampling

  - 연관된 question과 passage 간의 dense embedding 거리를 좁히는 것 (higher similarity) => Positive

  - 연관되지 않은 question과 passage간의 embedding 거리는 멀어야 함 => Negative

    ![a](https://user-images.githubusercontent.com/87477828/137332321-9943a859-fbb9-409a-98ea-e4d3c8fdc318.png)

- Dense Encoder 학습 목표와 학습 데이터 - Negative Sampling
  - Choosing negative examples:
    - Corpus 내에서 랜덤하게 뽑기
    - 좀 더 헷갈리는 negative 샘플들 뽑기 (ex. 높은 TF-IDF 스코어를 가지지만 답을 포함하지 않는 샘플)=> 모델 입장에서 구별하기 어려움=>정확도 높인 case가 많음

- Objective function

  - Positive passage에 대한 negative log likelihood (NLL) loss 사용=>softmax 사용했다는 것을 알 수 있다.

    ![a](https://user-images.githubusercontent.com/87477828/137333399-70053a03-dc90-45bd-9d8c-81b6a52ac2fd.png)

- Evaluation Metric for Dense Encoder (성능 측정)

  - Top-k retrieval accuracy : retrieve 된 passage 중에 답을 포함하는(최종 mrc) passage의 비율

    ![a](https://user-images.githubusercontent.com/87477828/137335904-49de1c1d-083b-4097-9655-b734c31792b4.png)

### 3. Passage Retrieval with Dense Encoder

- From dense encoding to retrieval

  - Inference : Passage와 query를 각각 embedding 한 후, query로부터 가까운 순서대로 passage의 순위를 매김

    ![a](https://user-images.githubusercontent.com/87477828/137336376-040b67a0-febd-4a07-9be1-7de5ac318cdb.png)

  - Retriever를 통해 찾아 낸 Passage을 활용, .MRC (Machine Reading Comprehension) 모델로 답을 찾음

    ![a](https://user-images.githubusercontent.com/87477828/137336814-74784c19-ac11-47f8-ab20-18c6fd417d78.png)

- How to make better dense encoding
  - 학습 방법 개선 (eg. DPR 논문 참고!)
  - 인코더 모델 개선 (BERT보다 큰, 정확한 Pretrained 모델)
  - 데이터 개선 (더 많은 데이터, 더 좋은 전처리 등)

- MRC Model도 중요하지만 좋은 문서를 찾아오는 것도 중요함.