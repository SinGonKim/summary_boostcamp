## Lecture 3: Generation-based MRC

### 1. Generation-based MRC

- Generation-based MRC 문제 정의

  - MRC 문제를 푸는 방법

    - Extraction-based mrc: 지문(context) 내 답의 위치를 예측 => (token) 분류 문제 (classification)

      ![1](https://user-images.githubusercontent.com/87477828/137243291-6f0cd48b-746b-4e62-95bd-2758d6a24b98.png)

      - Generation-based mrc: 주어진 지문과 질의 (question) 를 보고 답변을 생성 => 생성 문제(generation)

        ![2](https://user-images.githubusercontent.com/87477828/137243294-e30e3324-b66b-4543-8bc6-b59ed8c52c13.png)

        - Extraction-based-mrc =>Generation-based-mrc 는 쉽다!
        - Generation-based-mrc => Extraction-based-mrc는 어려울 수 있다!(답이 지문내에 없을 수도 있다!)

- Generation-based MRC 평가 방법

  - 동일한 extractive answer datasets => Extraction-based MRC와 동일한 평가 방법을 사용(recap)

    - Exact Match (EM) Score

      - EM = 1 when (Characters of the prediction) == (Characters of ground-truth)

        Otherwise, EM = 0

    - F1 Score

      - 예측한 답과 ground-truth 사이의 token overlap을 F1으로 계산

        ![3](https://user-images.githubusercontent.com/87477828/137244718-7febbf33-9924-4e92-bf63-b1495dec7dd2.png)

  ![1](https://user-images.githubusercontent.com/87477828/137248598-b1f9f519-9c75-491f-8d8a-9b528ab4499b.png)

  ​												- Generation-based MRC Model은 정답까지 생성해주는 박스라고 보면 된다.

- Generation-based MRC & Extraction-based MRC 비교

  - MRC 모델 구조
    - Seq-to-Seq PLM(Pre-trained language model) 구조 (generation) vs. PLM(Pre-trained language model) + Classifier 구조 (extraction)

  - Loss 계산을 위한 답의 형태 / Prediction의 형태

    - Free-form text 형태 (generation) vs. 지문 내 답의 위치 (extraction)

      => Extraction-based MRC: F1 계산을 위해 text로의 별도 변환 과정이 필요

      ![1](https://user-images.githubusercontent.com/87477828/137249891-f124e5f1-e0a0-4dfd-85f8-e0f7ba30c68c.png)

### 2. Pre-processing

- 입력표현 -데이터 예시

  ![1](https://user-images.githubusercontent.com/87477828/137250150-e168427b-f8fd-4fc2-b15b-4a6adfdf2929.png)

- 입력표현- 토큰화

  - Tokenization(토큰화) : 텍스트를 의미를 가진 작은 단위로 나누는 것(형태소)
  - Extraction-based MRC와 같이 WordPiece Tokenizer를 사용함
    - WordPiece Tokenizer 사전학습 단계에서 먼저 학습에 사용한 전체 데이터 집합(코퍼스)에 대해서 구축이 되어있어야 함.
    - 구축 과정에서 미리 각 단어 토큰들에 대해 순서대로 번호(인덱스)를 부여해둠

  - Tokenizer은 입력 텍스트를 토큰화한 뒤, 각 토큰을 미리 만들어둔 단어 사전에 따라 인덱스로 변환함.

  - WordPiece Tokenizer 사용예시

    - 질문 : '미국 군대 내 두번째로 높은 지위는 무엇인가?'
    - 토큰화 된 질문 : ['미국', '군대' ,'내' ,'두번째', '##로', '높은', '직', '##위는', '무엇인가', '?']
    - 인덱스로 바뀐 질문 : [101, 23545, 8910, 14423, 8996, 9102, 48506, 11261, 55600, 9707, 19855, 11018, 9294, 119137, 12030, 11287, 136, 102] (글자마다 각 인덱스 존재)

    => 인덱스로 바뀐 질문은 보통 input_ids(또는 input_token_ids)로 부름

    =>모델의 기본 입력은 input_ids만 필요하나, 그 외 추가적인 정보가 필요함 ex) `special token`, `attention mask`

- 입력표현 - Special Token

  - 학습시에만 사용되며 단어 자체의 의미는 가지지 않는 특별한 토큰

    - SOS(Start Of Sentence), EOS(End Of Sentence), CLS, SEP, PAD, UNK... 등등

      - Extraction-based MRC에선 CLS, SEP, PAD 토큰을 사용
      - Generation-based MRC에서도 PAD토큰은 사용됨, CLS, SEP 토큰 또한 사용할 수 있으나 대신 자연어를 이용하여 정해즌 텍스트 포맷(format)으로 데이터를 생성한다.(이 말은 경우에 따라 question, context로 대체하여 사용한다는 것이다. 사용하는 모듈에 따라 다르다는 것이다.)

      ![1](https://user-images.githubusercontent.com/87477828/137253341-89615b97-fbda-4ced-9f25-bad9185b228d.png)

- 입력표현 - additional information

  - Attention mask

    - Extraction-based MRC와 똑같이 어텐션 연산을 수행할 지 결정하는 결정하는 어텐션 마스크 존재

      (한마디로 무시할 단어를 표시하는 것이다.)

  - Token type ids

    - BERT와 달리 BART에서는 입력 시퀀스에 대한 구분이 없어 token_type_ids 가 존재하지 않음
    - 따라서 Extraction-based MRC와 달리 입력에 token_type_ids가 들어가지 않음

    ![2](https://user-images.githubusercontent.com/87477828/137253345-7daed561-71ef-486e-bed8-9d5702475255.png)

    ​		- <SEP> 같은 토큰이 주어지면 직접적으로 문장을 구분하고 좋지만 굳이 필요없다는 측면에서 없앤 것으로 추정됨

- 출력 표현 -정답 출력

  - Sequence of token ids

    - Extraction-based MRC에선 텍스트를 생성해내는 대신 시작/끝 토큰의 위치를 출력하는 것이 모델의 최종 목표였음

    - Generation-based MRC는 그보다 조금 더 어려운 실제 텍스트를 생성하는 과제를 수행

    - 전체 시퀀스의 각 위치 마다 모델이 아는 모든 단어들 중 하나의 단어를 맞추는 classification 문제

      ![1](https://user-images.githubusercontent.com/87477828/137254871-b44f4b23-e38d-48b9-a53b-aba252a1fb21.png)

    ![1](https://user-images.githubusercontent.com/87477828/137254987-2af8b513-f1e9-4de6-aea2-d54d280e9aac.png)

    - 단어를 하나씩 생성해서 붙여서 최종 답안을 만들어 예측

### 3. MODEL

- BART

  - 기계 독해, 기계 번역, 요약, 대화 등 sequence to sequence 문제의 pre-training을 위한 denoising autoencoder

    ![1](https://user-images.githubusercontent.com/87477828/137255393-2d324dcc-49e0-4ffb-9712-de870fa39ba3.png)

- BART Encoder & Decoder

  - BART의 인코더는 BERT처럼 bi-directional

  - BART의 디코더는 GPT처럼 uni-directional(autoregressive)

    ![1](https://user-images.githubusercontent.com/87477828/137255517-65e4b6f6-17c0-4a1b-a0a1-0e3503b8c519.png)

- Pre-training BART

  - BART는 텍스트에 노이즈를 주고 원래 텍스트를 복구하는 문제를 푸는 것으로 pre-training 함

    ![1](https://user-images.githubusercontent.com/87477828/137255732-ea6506dc-d4bb-48c8-bdc4-823414fc4275.png)

    ![1](https://user-images.githubusercontent.com/87477828/137255892-401ceca1-aa6f-439a-bb6b-01a4e7cc8004.png)

## 4. Post- processing

- Decoding

  - 디코더에서 이전 스텝에서 나온 출력이 다음 스텝의 입력으로 들어감 (autoregressive)

    - 다음단어 예측을 해야하기 때문이다.

  -  맨 처음 입력은 문장 시작을 뜻하는 스페셜 토큰

    ![1](https://user-images.githubusercontent.com/87477828/137256110-46864bac-b68f-42fb-9cf6-b28dd5d1b4c6.png)

![2](https://user-images.githubusercontent.com/87477828/137256115-82871002-b0e2-452e-90cd-9cf72ccb3a9d.png)

