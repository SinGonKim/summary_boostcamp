## Lecture 2: Extraction-based MRC

### 1. Extraction-based MRC

```markdown
### Extraction-based MRC 문제 정의
 질문(question)의 답변(answer)이 **항상 주어진 지문(context)내에** span으로 존재
 ex) SQuAD, KorQuAD, NewsQA, Natural Questions, etc
```

![1](https://user-images.githubusercontent.com/87477828/137066034-d5ac30b7-155a-4843-9ab2-8e739115d029.png)

- [Extraction-based MRC 문제 정의](https://hugginface.co/datasets)

- Extraction-based MRC 평가 방법

  - Exact Match (EM) Score
    - 예측값과 정답이 캐릭터 단위로 완전히 똑같을 경우에만 1점 부여. 하나라도 다른 경우 0점.

  ![1](https://user-images.githubusercontent.com/87477828/137066386-6500ade2-735a-4f85-baed-8b5c246546b8.png)

  - F1 Score
    - 예측값과 정답의 overlap을 비율로 계산.
    - 0점과 1점사이의 부분점수를 받을 수 있음

  ![2](https://user-images.githubusercontent.com/87477828/137066387-36f661c0-b4af-4e2b-ba04-b0b29bf8ec39.png)

  ![3](https://user-images.githubusercontent.com/87477828/137066389-841b200d-dc45-48e0-afbf-d8272bbe9fe6.png)

- Extraction-based MRC Overview

  ![1](https://user-images.githubusercontent.com/87477828/137066618-81246b57-356b-4714-a7a4-b06e6d37262d.png)

### 2. Pre-processing

- 입력 예시

  ![1](https://user-images.githubusercontent.com/87477828/137066733-64c6f1da-dbe6-455c-9bea-a3c9aa53a4e1.png)

- **Tokenization**

  - 텍스트를 작은 단위(Token)으로 나누는 것
    - 띄어쓰기 기준, 형태소, subword 등 여러 단위 토큰 기준이 사용됨
    - 최근에는 Out-Of-Vocabulary (OOV) 문제를 해결해주고 정보학적으로 이점을 가진 Byte Pair Encoding(BPE)을 주로 사용함
    - 본강에선 BPE 방법론중 하나인 WordPiece Tokenizer를 사용

  - `WordPiece Tokenizer` 사용 예시
    - "미국 군대 내 두번째로 높은 직위는 무엇인가?" => ['미국', '군대', '내', '두번째', '##로', '높은', '직', '##위는', '무엇인가', '?']
      - 자주 나오는 단어 '미국', '군대'는 잘 나뉘어진 반면 직위와 같이 자주 나오지 않는 단어는 잘 나뉘어지지 않았다.

- Special Tokens

  - Special Token을 통해 Question과 Context를 구별

    ![1](https://user-images.githubusercontent.com/87477828/137067357-d87ce457-72db-4e64-94d6-9b9deb1d01d1.png)

    ![2](https://user-images.githubusercontent.com/87477828/137067360-abc65b0c-9141-4c53-860b-926516a4949f.png)

    - CLS : 편의상 계속 처음에 사용하는 토큰

- Attention Mask

  - 입력 시퀀스 중에서 attention을 연산할 때 무시할 토큰을 표시
  - 0은 무시, 1은 연산에 포함
  - 보통 [PAD]와 같은 의미가 없는 특수토큰을 무시하기 위해 사용

  ![1](https://user-images.githubusercontent.com/87477828/137067646-878e54f4-b8fe-400f-9bd9-cd7abef886c2.png)

- Token Type IDs

  - 입력이 2개이상의 시퀀스일 때(예: 질문&지문), 각각에게 ID를 부여하여 모델이 구분해서 해석하도록 유도. ([SEP]토큰으로도 하는 방법 위에 소개)

  ![1](https://user-images.githubusercontent.com/87477828/137067786-2c48934f-a6d2-4433-b0ba-713691ca2cd2.png)



- 모델 출력값

  - 정답은 문서내 존재하는 연속된 단어토큰(span)이므로, span의 시작과 끝 위치를 알면 정답을 맞출 수 있음.

  - Extraction-based에선 답안을 생성하기 보다, 시작위치와 끝위치를 예측하도록 학습함. 즉 Token Classification문제로 치환

    ![1](https://user-images.githubusercontent.com/87477828/137067966-b769e3dc-7f81-4917-9fa6-ac989976beb0.png)

### 3. Fine-tuning

![1](https://user-images.githubusercontent.com/87477828/137068124-e97beca8-5024-4918-96a0-95daa9a5a226.png)



- Fine-tuning BERT

  ![1](https://user-images.githubusercontent.com/87477828/137068240-9a916300-f8e0-43fb-91ce-d308c72dfb40.png)



### 4. Post-processing

- 불가능한 답 제거하기

  - 다음과 같은 경우 candidate list에서 제거
    - End position이 start position보다 앞에 있는 경우(eg. start = 90,, end = 80)
    - 예측한 위치가 context를 벗어난 경우(eg. question 위치쪽에 답이 나온 경우)
    - 미리 설정한 max_answer_length 보다 길이가 더 긴 경우

  - 최적의 답안 찾기
    1. Start/end position prediction에서 score (logits)가 가장 높은 N개를 찾는다.
    2. 불가능한 start/end 조합을 제거한다.
    3. 가능한 조합들을 score의 합이 큰 순서대로 정렬한다.
    4. Score가 가장 큰 좋바을 최종 예측으로 선정한다.
    5. Top-k가 필요한 경우 차례대로 내보낸다.

### 실습

```python
## in colab
!nivda-smi # 사용하고 있는 gpu 확인
!pip install transformers ==4.4.1
!pip install datasets == 1.4.1
### 저작권으로 생략
```

