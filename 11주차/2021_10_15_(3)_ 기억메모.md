

## Lecture 7 : Linking MRC and Retrieval

### 1. Introduction to Open-domain Question Answering (ODQA)

- Linking MRC and Retrieval : Open-domain Question Answering (ODQA)

  - MRC : 지문이 주어진 상황에서 질의응답

    ![a](https://user-images.githubusercontent.com/87477828/137419965-6f9f1007-c602-4c6e-a865-fe6b287e4d6a.png)

  

  - QDQA : 지문이 따로 주어지지 않음. 방대한 World Knowledge에 기반해서 질의응답

    ​	![a](https://user-images.githubusercontent.com/87477828/137420137-f6ea3c95-d3bc-435a-b6d1-1711170799ae.png)

    - EX) Modern search engines: 연관문서 뿐만 아니라 질문의 답을 같이 제공

      ![a](https://user-images.githubusercontent.com/87477828/137420260-1bf859ec-0458-4404-a5a6-f771d3c20660.png)

- History of ODQA

  - Text retrieval conference (TREC) - QA Tracks (1999-2007) : 연관문서만 반환하는 information retrieval(IR)에서 더 나아가서, short answer with support 형태가 목표

    1) Question processing + 2. Passage retrieval + 3. Answering processing

       ![a](https://user-images.githubusercontent.com/87477828/137420516-342b3ec0-ffff-4726-96df-a0758489aec4.png)

    1. Question processing

       Query formulation: 질문으로부터 키워드를 선택/ Answer type selection (ex. Location: country)

       ![a](https://user-images.githubusercontent.com/87477828/137420773-eee062fd-c9e1-47b7-a3a3-0e71697079d4.png)

    2. Passage retireval: 기존의 IR 방법을 활용해서 연관된 document를 뽑고, passage 단위로 자른 후 선별 (Named entity / Passage 내 question 단어의 개수 등과 같은 hand-crafted features 활용)

       ![a1](https://user-images.githubusercontent.com/87477828/137420775-6bf4ca3d-c6ba-4fe6-ab4e-b58773d38ab9.png)

    3. Answer processing

       Hand-crafted features와 heuristic을 활용한 classifier

       주어진 question과 선별된 passage들 내에서 답을 선택

       최근 mrc는 passage를 선별할 뿐만 아니라 passage내의 어떤 span이 답이 되는 맞추는 것까지 진화

       ![a2](https://user-images.githubusercontent.com/87477828/137420776-16b5da25-bf20-4e5a-b130-b9362935ab2d.png)

  - IBM Watson(2011)
    - The DeepQA Project
    - Jeopardy! (TV quiz show) 우승

  - Recent ODQA Research

    - 여러 모델이 생기고 발전이 생김

    ![a](https://user-images.githubusercontent.com/87477828/137421130-7be85b74-5832-4246-83b4-7ba1baaab0c3.png)

### 2. Retriever-Reader Approach

- Retriever-Reader 접근 방식

  - Retriever: 데이터베이스에서 관련있는 문서를 검색(Search) 함

    - 입력
      - 문서셋(Document corpus)
      - 질문(query)
    - 출력
      - 관련성 높은 문서(document)

  - Reader: 검색된 문서에서 질문에 해당하는 답을 찾아냄

    - 입력
      - Retrieved된 문서(document)
      - 질문(query)
    -  출력
      - 답변(answer)

    ![a](https://user-images.githubusercontent.com/87477828/137421915-513c2954-583f-43a7-969c-5bf64c86e25d.png)

- 학습 단계

  - Retriever:
    - TF-IDF, BM25 -> 학습 없음(일반적인 labed된 형태의 학습방법은 없음)
    - Dense -> 학습 있음(QA 데이터셋을 통해서)

  - Reader:
    - SQuAD와 같은 MRC 데이터셋으로 학습
    - 학습 데이터를 추가하기 위해서 Distant supervision 활용

- Distant supervision

  - 질문-답변만 있는 데이터셋(CuratedTREC, WebQuestions, WikiMovies)에서 MRC 학습데이터 만들기. Supporting document가 필요함.
    1. 위키피디아에서 Retriever를 이용해 관련성 높은 문서를 검색
    2. 너무 짧거나 긴 문서, 질문의 고유명사를 포함하지 않는 등 부적절한 문서 제거
    3. answer가 exact match로 들어있지 않은 문서 제거
    4. 남은 문서 중에 질문과 (사용 단어 기준) 연관성이 가장 높은 단락을 supporting evidence로 사용함

  - 각 데이터셋 별 distant supervision을 젹용한 예시

    ![a](https://user-images.githubusercontent.com/87477828/137425807-fa84db65-4277-482c-8191-99b1d53167aa.png)

- Inference
  - Retriever가 질문과 가장 관련성 높은 5개 문서 출력
  - Reader는 5개 문서를 읽고 답변 예측
  - Reader가 예측한 답변 중 가장 score가 높은 것을 최종 답으로 사용함.

### 3. Issues & Recent Approaches

- Different Granularities of text at indexing time

  - 위키피디아에서 각 Passage의 단위 문서, 단락, 또는 문장으로 정의할지 정해야함.

    - Article: 5.08 million

    - Paragraph: 29.5 million

    - Sentence: 75.9 million

      ![a](https://user-images.githubusercontent.com/87477828/137426153-a429f5d4-ef31-46fb-a4dd-d1e859bf9338.png)

  - Retriever 단계에서 몇 개(top-k)의 문서를 넘길지 정해야 함

  - Granularity(세분화)에 따른 k가 다를 수 밖에 없음.

    - eg. article->k=5. paragraph -> k=29, sentence, -> k=78 (대부분 k를 올리면 성능이 좋아지지만 항상 그런 것은 아니다.)

      ![a1](https://user-images.githubusercontent.com/87477828/137426155-4447afe2-2cc3-42d8-af47-86c3a70e694c.png)

- `Single-passage training` vs `Multi-passage training`

  - (Single-passage) : 현재 우리는 k개의 passages 들을 reader이 각각 확인하고 특정 answer span에 대한 예측 점수를 나타냄. 그리고 이 중 가장 높은 점수를 가진 answer span을 고르도록함.
    - 이 경우 각 retrieved passages 들에 대한 직접적인 비교라고 볼 수 없음
    - 따로 reader 모델이 보는 게 아니라 전체를 한번에 보면 어떨까?

  - (Multi-passage) : retrieved passages 전체를 하나의 passage로 취급하고, reader 모델이 그 안에서 answer span 하나를 찾도록 함.

    <u>Cons</u>: 문서가 너무 길어지므로 GPU에 더 많은 메모리를 할당해야함 & 처리해야하는 연산량이 많아짐

- Importance of each passage

  - Retriever 모델에서 추출된 top-k passage들의 retriefval  score를 reader 모델에 전달.

    ![a2](https://user-images.githubusercontent.com/87477828/137426156-1c7bef25-acde-4f4d-a794-0ba27ada36ce.png)

