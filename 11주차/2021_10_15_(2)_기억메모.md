## Lecture 6: Passage Retrieval - Scaling Up

### 1. Passage Retrieval and Similarity Search

- MIPS(Maximum Inner Product Search)

  - 주어진 질문(query) 벡터 q에 대해 Passage 벡터 v들 중 가장 질문과 관련된 벡터를 찾아야함

    - 여기서 관련성은 내적(inner product)이 가장 큰 것

      ![a](https://user-images.githubusercontent.com/87477828/137409348-5ec7920a-e852-4d82-9689-39a645b1e96a.png)

    - 4, 5 강에서 사용한 검색 방법 : brute-force (exhaustive) search
      - 저장해둔 모든 Sparse/Dense 임베딩에 대해 일일히 내적값을 계산하여 가장 큰 값이 큰 passage를 추출

    ![a](https://user-images.githubusercontent.com/87477828/137409579-af7aad4e-6de6-4008-8ceb-7b9c30ba797c.png)

- MIPS & Challenges

  - 실제로 검색해야할 데이터는 훨씬 방대함

    - 5백만 개(위키피디아)
    - 수십 억, 조 단위까지 커질 수 있음

    => 따라서 더이상 모든 문서 임베딩을 일일히 보면서 검색할 수 없음

- Tradeoffs of similarity search

  - Search Spped
    - 쿼리(질문) 당 유사한 벡터를 k개 찾는데 얼마나 걸리는지?
      - 가지고 있는 벡터량이 클 수록 더 오래걸림

  - Memory Usage
    - 벡터를 사용할 때, 어디에서 가져올 것 인지?
      - RAM에 모두 올려둘 수 있으면 빠르지만, 많은 RAM용량을 요구함
      - 디스크에서 계속 불러와야한다면 속도가 느려짐

  - Accuracy
    - brute-force 검색 결과와 얼마나 비슷한지?
      - 속도를 증가시키려면 정확도를 희생해야하는 경우가 많음

  ![a](https://user-images.githubusercontent.com/87477828/137410331-f7a2b2e4-47f7-47bb-a79e-15639e69308d.png)

  - Exhaustive search : 어떻게 검색을 효율적으로 할 것인가에 대해서 고민이 필요

    - Compression : 어떻게 적은용량으로 저장하여 사용할 것인가에 대한 고민
    - Pruning : inference 때 Pruning 사용

  속도와 재현율(recall)의 관계

   - 더 정확한 검색을 하려면 더 오랜 시간이 소모됨

     ![a](https://user-images.githubusercontent.com/87477828/137410550-9c0f4d2f-36e6-4255-b347-59985fce7b17.png)

- Increasing search space by bigger corpus

  - 코퍼스(corpus)의 크기가 커질 수록

    - 탐색 공간이 커지고 검색이 어려워짐

    - 저장해 둘 Memory space 또한 많이 요구가 됨

    - Sparase Embedding의 경우 이런 문제가 훨씬 심함

      예) 1M docs, 500K distinct terms = 2TB

      ![a](https://user-images.githubusercontent.com/87477828/137413976-a3e66851-3009-4e53-a5bb-3129c91b200a.png)

### 2. Approximating Similarity Search

- Compression -Scalar Quantization (SQ) (압축- 스칼라 양자화)

  - Compression : vector를 압축하여, 하나의 vector가 적은 용량을 차지 => 압축량 증가 => 메모리 사용량 감소, 정보손실 증가

    ex) Scalar quantization: 4-byte floating point => 1-byte (8bit) unsigned integer로 압축

    ![a](https://user-images.githubusercontent.com/87477828/137414390-c490d3ce-9728-45ff-a631-30946c95c290.png)

- Pruning - Inverted File (IVF)

  - Pruning - Search space를 줄여 search 속도 개선 (dataset의 subset만 방문)

    => Clustering + Inverted file을 활용한 search

    1) Clustering: 전체 vector space를 k개의 cluster로 나눔 (ex. k-means clustering)

       ![a](https://user-images.githubusercontent.com/87477828/137414983-e463c531-5dd9-4c93-bbd0-bbec5fc100e0.png)

    2. Inverted file (IVF)

       : Vector의 index = inverted list structure

       => (각 cluster의 centroid id)와 (해당 cluster의 vector들)이 연결되어있는 형태

       ![a](https://user-images.githubusercontent.com/87477828/137415426-f15e55b3-b302-45c0-8cda-d0a7dd8a1f53.png)

  - Searching with clustering and IVF

    - 주어진 query vector에 대해 근접한 centroid 벡터를 찾음
    - 찾은 cluster의 inverted list 내 vector 들에 대해 서치 수행

    ![a](https://user-images.githubusercontent.com/87477828/137415739-62415e1a-99b8-4fc6-99f3-91a592159200.png)

### 3. Introduction to FAISS

- What is FAISS

  - [FAISS_github](https://github.com/facebookresearch/faiss)

  - [FAISS_tutorial](https://github.com/facebookresearch/faiss/tree/master/tutorial/python)

    ![a](https://user-images.githubusercontent.com/87477828/137416602-e5f1a2a2-8a42-4312-a29e-65ef19a3b21b.png)

  - FAISS: Library for efficient similarity search

    ![a](https://user-images.githubusercontent.com/87477828/137416772-8e3d3e6d-526b-4240-b40f-a7fd914d0522.png)

- Passage Retrieval with FAISS

  - Train index and map vectors

    ![a](https://user-images.githubusercontent.com/87477828/137417024-aacd72b9-bf47-4f3b-b16e-862a3ff99fa8.png)

  - Search based on FAISS index

    ![a1](https://user-images.githubusercontent.com/87477828/137417028-61bf3815-23fd-4f98-acfa-307cc004d422.png)

    - nprobe : 몇 개의 가장 가까운 cluster를 방문하여 search 할 것인지

### 4. Scaling up with FAISS

- FAISS Basics

  - brute-force로 모든 벡터와 쿼리를 비교하는 가장 단순한 인덱스 만들기

    - 준비하기

      ![a](https://user-images.githubusercontent.com/87477828/137418314-e1b2f878-b01f-4574-a543-e7dd8bd22230.png)

    - 인덱스 만들기

      ![a1](https://user-images.githubusercontent.com/87477828/137418318-4dda0578-b8de-48ce-b28a-8d4c74c1892f.png)

    - 검색하기

      ![a2](https://user-images.githubusercontent.com/87477828/137418320-8de8f70c-3037-4d40-b493-4b5be129facb.png)

- IVF with FAISS

  - IVF 인덱스 만들기
  - 클러스터링을 통해서 가까운 클러스터 내 벡터들만 비교함
  - 빠른 검색 가능
  - 클러스터 내에서는 여전히 전체 벡터와 거리비교 (Flat)

  IVF 인덱스

  ![a](https://user-images.githubusercontent.com/87477828/137418903-3484dcba-87db-4249-8831-1d3c680f568d.png)



- IVF-PQ with FAISS

  - 벡터 압축 기법 (PQ) 활용하기
  - 전체 벡터를 저장하지 않고 압축된 벡터만을 저장
  - 메모리 사용량을 줄일 수 있음

  IVF-PQ 인덱스

  ![a](https://user-images.githubusercontent.com/87477828/137419138-7806854e-30e1-4242-bec0-52e3c6ebf614.png)

-  Using GPU with FAISS

  - GPU의 빠른 연산 속도를 활용할 수 있음
    - 거리 계산을 위한 행렬곱 등에서 유리

  - 다만, GPU 메모리 제한이나 메모리 random access 시간이 느린 것 등이 단

  단일 GPU 인덱스

  ![a1](https://user-images.githubusercontent.com/87477828/137419140-37f0308c-02e8-471a-b268-5eced0e47cbb.png)

- Using Multiple GPUs with FAISS

  - 여러 GPU를 활용하여 연산 속도를 한층 더 높일 수 있음

  다중 GPU 인덱스

  ![a2](https://user-images.githubusercontent.com/87477828/137419141-6dab6697-209a-41fa-be78-f98cced541b1.png)

