# BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding



## Abstract

- BERT = Bidirectional Encoder Representations from Transformers
- BERT는 모든 계층(layers)에서 양방향으로 문맥을 고려하여 정답(label)이 부착되지 않은 문장에서 깊이 양방향으로 사전에 훈련받도록 설계되었다.
  - 그 결과 BERT모델은 추가적인 하나의 결과 계층(output layer)에서 세밀하거 조정이 가능해 다양한 넓은 분야 업무에서 최첨단 성능을 달성할 수 있었다.(QA나 language inference를 예로 들수 있는데 특정 업무에 따라 아키텍쳐 수정 없이 사용하여 나온 결과이다.)

- 논문 발표할 당시 BERT는 11개의 업무에서 state-of-the-art(최첨단)의 성능을 달성했다.

  

## 1. Introduction

- ~~첫 문장은 Language model pre-trainning이 많은 발달로 abstract에서 나온 역할과 그 외 많은 일을 할 수 있다는 내용이여서 삭제~~
- NLP 관련 하위 작업들(down-stream-tasks)에 사전 훈련된 언어 표현을 적용하기 위한 두 가지 기존 전략이 있는데 기능 기반(feature-based)과 미세 조정(fine-tuning)이 그 것이다.
  - feature-based 접근방법은 ELMO와 같다.('-사전 훈련된 표현을 추가 기능으로 포함하는 작업별 아키텍처를 사용합니다.' 라고 파파고 번역을 치면 나오는데 솔직하게 이해를 제대로 하지 못한 것 같습니다. 머리속에 안그려집니다... ELMO에 대해서 좀 더 공부해야 할 것 같습니다. ㅠㅠ-)
  - fine-tuning approach는 GPT와 같다. (각 작업별 최소한의 파라미터를 두고 훈련을 시킨다는 것 같습니다.)
  -  general language(영어?) representations을 학습하기 위해 undirectional language models를 사용할 때 pre-training 과정동안 두 접근 방법은 같은 목적함수를 공유합니다. -> ~~bidirectional를 가진 BERT를 자랑하기 위해 기존 GPT-1을 까는 내용이 나오겠다고 생각했는데 바로 밑 문장에 나왔다.~~ undirection 비판

- BERT는 크게 2개의 Task를 만들어 학습을 진행했다. 문장에서 랜덤으로 몇 개의 token을 가리고 주변 문맥으로 해당 token을 맞추는 task인 Masked Language Model(MLM)과 연이은 문장이 연관된 쌍(pair)인지, 랜덤으로 매칭시킨 쌍(pair)인지 구분하는 task인 Next Sentence Prediction(NSP)에 대해 학습을 진행시켰다. 이 논문의 기고문은 다음과 같다.

  - 언어 표현 모델에서 양방향 사전훈련의 중요성을 입증한다.

  - 사전 교육된 표현으로 인해 고도로 설계된 많은 작업별 아키텍처의 필요성이 줄어듦을 알 수 있다.

  - BERT는 11개의 NLP 작업에서 최첨단 결과(최고 성적)을 얻믕으로써 기술을 발전 시킨다.

    

## 2. Related work

영어 표현(general language representation) 사전 훈련은 오랜 역사가 있다.

- 2.1 Unsupervised Feature-based Approahces
- 2.2 Unsupervised Fine-tuning Approaches
- 2.3 Transfer Learning from supervised Data

솔직히 요약을 못하겠다. 계속 번역만 하는 것 같고...  이 논문의 중요 내용은 아니라고 생각한다.





## 3. BERT

- BERT 에는 두 스텝이 있다. pre-training과 fine-tuning
  - 사전 훈련(pre-training)동안 모델은 정답이 없는 다른 사전훈련(pre-training) 일들을 거쳐 훈련 받는다.
  - 미세 조정(fine-tuning)경우, BERT 모델은 먼저 사전 훈련된 매개 변수로 초기화되며, 모든 매개 변수는 하위 작업(down-stream- task)의 정답이 있는 데이터를 사용하여 미세조정된다.

- BERT의 특색있는 특징들은 여러 작업에 걸쳐 통합된 아키텍처이다.

  

### Model Architecture

- BERT의 모델 아키텍처는 Vaswani et al.(2017) 과 tensor2tensor library에 나왔던 설명된 원래 구현을 기반으로 하는 다중 계층 양방향 트랜스포머 인코더이다.
- 모델 사이즈가 두 가지 있다. BERT(BASE) = (L = 12, H = 768, A = 12, Total Parameters = 110M) 과 BERT(LARGE) = (L = 24, H = 1024, A = 16, Total Parameters = 340M)
  - L = the number of layers (즉 Transformer blocks)
  - H = the hidden size
  - A = the number of self-attention heads

- BERT(BASE)는 목적이 GPT-1과 비슷해 model_size가 같다.
- 가장 중요한(결정적으로) BERT Transformer은 양방향 self-attention을 사용하는 반면, GPT Transformer은 모든 토큰이 왼쪽에 있는 문맥에만 관여를 하여 제한된 self-attention을 사용한다는 것이다.

### Input/output representations

-  WordPiece 임베딩을 사용한다.
- [CLS]를 문장의 시작을 알리는 용도로 사용한다.
- 문장 쌍은 하나의 스퀀스(sequence)로 함께 사용된다. 문장쌍을 구별 하는 방법은 두가지 있다.
  - [SEP]라는 특별한 Token을 추가하여 문장의 구분 목적으로 사용한다.
  - 그것이 문장 A에 속하는지 문장 B에 속하는지 나타내는 모든 토큰에 학습된 임베딩을 추가한다.

### 3.1 Pre-training BERT

#### Task #1: Masked LM

- 기존 a deep bidirectional model은 강력했지만 문제점이 있었는데 표준 조건부 언어 모델은 양방향 조건화가 각 단어를 간접적으로 "자신을 볼 수 있도록" 허용하고 모델은 다중 계층 문장에서 대상 단어를 단계적으로 예측할 수 있기 때문에 왼쪽에서 오른쪽으로 혹은 오른쪽에서 왼쪽으로 훈련 될 수 있었다.

- 따라서 deep bidirectional representation을 훈련시키기 위해 일부 입력 토큰을 무작위로 마스킹한 다음 마스킹된 토큰을 예측하도록 하면 된다. 이 과정을 우리는 "masked LM"(MLM)이라 부른다.

  - 우리는 전체 실험에서 각 시퀀스에서 랜덤으로 15%의 wordpiece tokens 을 마스킹 시켰다.
  - 자동 인코더의 노이즈 제거와는 대조적으로 우리는 전체 입력을 재구성하기 보다는 마스킹된 단어만 예측한다.
  - i 번째 token이 마스킹 되었을 때 우리는 i번째 token을 3가지중 하나로 대체한다.
    - 마스킹 된 토큰중 80%를 [MASK]로 쓴다.
    - 10%는 다른 랜덤 토큰으로 변환시킨다.
    - 10%는 그대로 쓴다.

  - 그러면 T(i)는 교차 엔트로피 손실(cross entropy loss)을 이용해 원래 토큰을 예측하는데 사용된다.

#### Task #2: Next Sentence Prediction (NSP)

- Question Answering(QA)나 Natural Language Inference(NLI)와 같이 많은 중요한 하위 작업은  두 문장사이의 관계 이해를 중요하게 생각하는 것에 기반을 둔다.
- 문장 관계를 이해하는 모델을 훈련하기 위해 단일 언어 말뭉치에서 생성할 수 있는 이진화된 다음 문장을 예측 작업을 위해 사전 훈련을 한다.
  - 각 사전 훈련 예제에 대해 문장 A와 B를 선택할 때, B의 50%는 A 뒤에 오는 실제 다음 문장이고, 50%는 말뭉치의 무작위 문장이다.
  - 논문 [Figure1]에서 C는 다음 문장을 예측하는 것으로 사용된다.(NSP)

#### pre-training data

- 사전 훈련(pre-training) 말뭉치(corpus)로 우리는 BooksCorpus(800M words)와 English Wikipedia(2500M words)를 사용한다.
- 그 다음은 잘 모르겠다...

#### Fine-tuning

- Token 정보를 이용하는 Task에서는 Token Ouput, 문장의 정보를 이용하는 Task에서는 [CLS] Token Output에 대부분 한 번의 Fully Connected Layer만 추가해 재학습시켰다.

## 4. Experiments

이 과정에서 BERT의 11개의 NLP task에서 fine-tuning 결과를 알 수 있다.

#### 4.1 GLUE

- GLUE = The General Language Understanding Evaluation
- GLUE 벤치마크는 다양한 자연어 이해 작업의 집합이다.
- GLUE의 데이터셋에 대한 것은 Appendix B.1에
- Batch_size = 32 그리고 fine-tune for 3 epochs를 모든 GLUE일들에 대한 데이터에 사용한다. (Why?)
- BERT Large의 경우 소규모 데이터셋에서 미세조정(fine-tuning)이 불안정하다. (why?)
- Dev set이 무엇인가?
- BERT(BASE)와 Open AI GPT-1은 attention masking과는 별개로 모델 아키텍처 측면에서 거의 동일하다. 

#### 4.2 SQuAD v1.1

- SQuAD = The Stanford Question Answering Dataset
- 10만개의 네티즌의 질문/답변 쌍의 집합이다.
  - 답을 포함하는 위키피디아의 질문과 구절을 주어진 과제는 그 구절의 답변 텍스트 범위를 예측하는 것이다. ->(RL을 첨가하면 텍스트를 참고하면 텍스트 범위를 예측하는 것이 아닌 응용해서 새로운 답이 나올 수 있을까?)

- 다음 내용은 반은 이해 반은 노이해라 적지 않겠다.

#### 4.3 SQuAD v2.0

- SQuAD v1.1에서 추가하여 단답 기능이 생겼다.
- 미세조정(fine-tuned)이 2 epochs로 learning rate=5e-5 그리고 batch_size = 48 
- 이전에 비해 +5.1 F1 개선되었다.

#### 4.4 SWAG

- SWAG = The Situations With Adversial Generations
- 문장이 주어졌을 때, 과제는 네 가지 선택 중에서 가장 그럴듯한 연속을 선택하는 것이다.
- 3epochs에 learning rate = 2e-5, batch_size = 16



## 5. Ablation Studies (경량화 연구?)

더 중요하고 관계를 잘 이해하기 위해 BERT의 많은 측면에 대해 연구



#### 5.1 Effect of pre-training Task

사전 훈련 데이터(pre-training data), 미세 조정 방식(fine-tuning scheme), 하이퍼 파라미터를 BERT(BASE)로 평가하여 BERT의 deep bidirectionality의 중요성을 보여준다.

- No NSP
  - MLM은 있지만 NSP 없이 시행

- LTR & No NSP
  - MLM 대신 Left to Right(LTR)로 (LM) 시행

- LTR 모델이 MLM에 비해 모든 부분에서 성능이 떨어졌는데 MRPC와 SQuAD에서 크게 떨어짐 (약 10%)
- LTR 시스템 강화를 위한 선의의 시도를 하기 위해 무작위로 초기화된 BiLSTM을 추가
  - SQuAD에서는 상당히 개선되었지만 나머지 부분에서 오히려 나빠짐

- ELMO처럼 LTR과 RTL 모델을 분리해서 각 토큰을 넣어 모델 합체
  - bidirectional model 보다 두 배 비쌈
  - QA와 같은 작입에서 비직관적
  - a deep bidirectional model에 비해 강력하지 못하다.

#### 5.2 Effect of Model Size

미세 조정 작업 정확도에 대한 모델 크기의 영향을 탐구한다.

- BERT 모델을 number of layers(L), hidden units(H), attention heads(A)를 다르게 해서 훈련을 시켜봄. (다른 하이퍼 파라미터나 훈련 과정은 이전과 동일하게 진행)
- 모델 사이즈가 큰 모델일 수록 4개의 데이터셋에 걸쳐 모두 강력하게 정확도가 증가하였다.(심지어 MRPC에서는 3600개의 정답이 있는 훈련자료만으로 훈련을 시켰다.)
- hidden dimension size의 경우 200에서 600으로 증가할 때는 상당이 도움이 되지만 1000개가 넘어가면 큰 개선이 되지 않는다.



#### 5.3 Feature- based Approach with BERT

잘 이해가 안됨..ㅠㅠ

-  사전 훈련된 모델에서 고정 특징을 추출하는 기능 기반 접근법은 일정한 차이점이 있다.
  - 모든 작업은 트랜스포머 인코더 아키텍처로 쉽게 표현할 수 있으므로 작업별 모델 아키텍처를 추가해야 한다.
  - 교육 데이터의 값비싼 표현을 한 번 미리 계산한 다음 이 표현 위에 더 저렴한 모델을 사용하여 많은 실험을 실행할 때 중요한 계산상의 이점이 있다.

- 미세 조정 접근 방식을 완화하기 위해 BERT의 매개 변수를 미세 조정하지 않고 하나 이상의 계층에서 활성화를 추출하여 기능 기반 접근 방식을 적용
  - 이러한 상황별 임베딩은 분류계층 전에 무작위로 초기화된 2개의 계층 768 차원 BiLSTM에 대한 입력으로 사용된다.

- BERT(LARGE)는 경쟁적으로 최고 성능을 나타내었다.

## 6. Conclusion

언어 모델을 이용한 transfer learning으로 인한 최근의 경험적 개선은 풍부한 unsupervised learning pre-training이 많은 언어 이해 시스템의 필수적인 부분임을 입증했다.

우리의 주요 기여는 이러한 발견을 deep bidirectional architecture로 더욱 일반화하여 동일한 사전 훈련된(pre-trained) 모델이 광범위한 NLP작업을 성공적으로 처리할 수 있도록 하는 것이다.



## Question

- 4.1 GLUE
  - Batch_size = 32 그리고 fine-tune for 3 epochs를 모든 GLUE일들에 대한 데이터에 사용한다. (Why?)
  - BERT Large의 경우 소규모 데이터셋에서 미세조정(fine-tuning)이 불안정하다. (why?)

- 4.2 SQuAD1.1
  - 답을 포함하는 위키피디아의 질문과 구절을 주어진 과제는 그 구절의 답변 텍스트 범위를 예측하는 것이다. ->(RL을 첨가하면 텍스트를 참고하면 텍스트 범위를 예측하는 것이 아닌 응용해서 새로운 답이 나올 수 있을까?)

- 4.3 SQuAD 2.0
  - 미세조정(fine-tuned)이 2 epochs로 learning rate=5e-5 그리고 batch_size = 48 은 실험으로 최적이라는 조건이 나온 것일까?

- 책을 통해 알게 된 것인데 BERT는 기존 word2vec형태의 Fixed Embedding과 달리, 문장의 문맥에 따라 Embedding Vector가 달라지는 특성 때문에 Contextual Embedding이라 표현하고 있다고 한다. 이 덕분에 서로 다른 문맥에서 사용되는 동음이의어 문제도 해결할 수 있었다고 하는데 어떻게 동음이의어를 해결 할 수 있다는지 자세히 알 고 싶다.

