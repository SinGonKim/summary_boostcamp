### GPU 사용시

- 내장된 cudnn 자동 튜너를 활성화할 때 쓰는 코드 2개

```python
# Randomness 제어를 위한 설정법
torch.backends.cudnn.deterministic = True 
torch.backends.cudnn.benchmark = False 

torch.backends.cudnn.determinstic = True -> #연산속도가 느려진다.
torch.backends.cudnn.benchmark = True -> # 하드웨어에 맞게 사용할 최상의 알고리즘을 찾는다.
# 입력이미지 크기가 자주 변하지 않으면 초기 시간이 소요되지만 일반적으로 더 빠른 런타임 효과를 볼 수 있다.
# 그러나 입력 이미지 크기가 반복될 때마다 변경된다면 런타임성능이 오히려 저하될 수 있다.
```

[What does torch.backends.cudnn.benchmark do?](https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936/7)

 - 두개의 코드를 쓰면 V100 GPU에서 충분히 정확도를 높이고 빠르게  훈련할 수 있다고 생각했는데 epoch training 하나하는데 8시간 넘게 걸렸다.(그래서 모델이 잘 못 되었나 싶었는데 이 문제가 가장 크다는 것을 알았다.)

 - 대회 보고서 마지막 화룡정점용으로 쓰는 것 아니면 좋은 GPU 여러개 아니면 쓸 일이 없을 것 같다.

    

## 하이퍼 파라미터 튜닝

- VGG-16 model로 성공한 두 그래프

  - ![VGG graph](https://user-images.githubusercontent.com/87477828/131382236-3d5520bc-7e5b-4caf-8ceb-b2fe1a2de7fe.png)

    ```python
    violate_transformations['train'] = transforms.Compose([
        Resize(),
        RandomRotation()
        CenterCrop()
        GaussianBlur()
        ColorJitter
        ToTensor
        Normalize
        ])
    pink_transformations['train'] = transforms.Compose([
        Resize()
        RandomResizedCrop()
        RandomHorizontalFlip
        RandomRotation()
        ColorJitter()
        ToTensor()
        Normalize()
    ])
    ```

    

    - 하이퍼파라미터

      ```python
      # violate
      criterion = nn.CrossEntropyLoss()
      optimizer = optim.Adam(model.linear_layers.parameters(), lr=0.01)
      #pink
      criterion = nn.CrossEntropyLoss()
      
      # 전이학습에서 학습시킬 파라미터를 params_to_update 변수에 저장
      params_to_update_1 = []
      params_to_update_2 = []
      params_to_update_3 = []
      
      
      #학습시킬 파라미터명
      update_param_names_1 = ["features"]
      update_param_names_2 = ["classifier.0.weight",
                              "classifier.0.biase", "classifier.3.weight",
                              "classifier.3.bias"]
      update_param_names_3 = ["classifier.6.weight", "classifier.6.bias"]
      update_param_names_4 = ["linear_layers"]
      #학습시킬 파라미터 외에는 경사를 계산하지 않고 변하지 않도록 설정
      for name, param in model.named_parameters():
          if update_param_names_1[0] in name:
              param.requires_grad = True
              params_to_update_1.append(param)
              print("params_to_update_1에 저장: ", name)
          elif name in update_param_names_2:
              param.requires_grad = True
              params_to_update_2.append(param)
              print("params_to_update_2에 저장: ", name)        
          elif name in update_param_names_3:
              param.requires_grad = True
              params_to_update_3.append(param)
              print("params_to_update_3에 저장: ", name) 
          elif name in update_param_names_4:
              param.requires_grad = True
              params_to_update_4.append(param)
              print("params_to_update_4에 저장: ", name)
          else:
              param.requires_grad =False
              print("경사 계산 없음. 학습하지 않음: ", name)
      
      #최적화 방법 설정
      optimizer = optim.SGD([
          {'params' : params_to_update_1, 'lr' :1e-4},
          {'params' : params_to_update_2, 'lr' :5e-4},
          {'params' : params_to_update_3, 'lr' :1e-3}
      ], momentum = 0.9)
      ```

      - 과적합 해결방법
        - weight decay 쓰니 조금 줄어들 것 같다.

      - 점수 높이기 위한 방안
        - 자료 부족 해결해야한다.
        - 전처리를 조금 더 공부해야할 듯 싶다.