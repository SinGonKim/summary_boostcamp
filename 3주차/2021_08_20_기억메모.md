## ch06 모델 불러오기

- Fine tuning : 이미 학습된 모델을 가져와서 데이터셋에 맞게 좀 더 학습하고 사용하는 것



- model.save()

  - 학습의 결과를 저장하기 위한 함수

  - 모델 형태(architecture)와 파라미터를 저장하는 2가지 방법 존재

  - 모델 학습 중간 과정의 저장을 통해 최선의 결과모델을 선택

  - 만들어진 모델을 외부 연구자와 공유해 학습 재연성 향상

    ```python
    for param_tensor in model.state_dict(): #state_dict: 모델의 파라미터를 표시
        print(param_tensor, "\t", model.state_dict()[param_tensor].size())
        
    torch.save(model.state_dict(),
               os.path.join(MODEL_PATHJ, "model.pt")) #모델의 파라미터를 저장
    
    
    new_model.load =torch.load(os.path.join(MODEL_PATH, "model.pt")) #모델을 불러올 때 사용
    ```

    

- Checkpoints
  - 학습의 중간 결과를 저장하여 최선의 결과를 선택
  - earlystopping 기법 사용시 이전 학습의 결과물을 저장
  - 일반적으로 epoch, loss와 metric 값을 지속적으로 확인 저장
  - colab에서 지속적인 학습을 위해 필요

![check](https://user-images.githubusercontent.com/87477828/130230076-76306a09-5de8-4224-974e-b410eb11f2d5.png)

- Transfer learning

  - 다른 데이터셋으로 만든 모델을 현재 데이터에 적용
  - 일반적으로 대용량 데이터셋으로 만들어진 모델의 성능향상
  - 현재의 딥러닝에서는 가장 일반적인 학습 기법
  - backbone architecture가 잘 학습된 모델에서 일부분만 변경하여 학습을 수행함
  - NLP는 HuggingFace를 백본으로 굉장히 많이 사용한다.

  

- Freezing

  - pretained model을 활용할 시 모델의 일부분을 중지시킴

![freezing](https://user-images.githubusercontent.com/87477828/130231792-ceb020f7-9fd0-4456-8629-02a8613e50f5.png)

```