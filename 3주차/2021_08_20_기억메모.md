## ch06 모델 불러오기

- Fine tuning : 이미 학습된 모델을 가져와서 데이터셋에 맞게 좀 더 학습하고 사용하는 것



- model.save()

  - 학습의 결과를 저장하기 위한 함수

  - 모델 형태(architecture)와 파라미터를 저장하는 2가지 방법 존재

  - 모델 학습 중간 과정의 저장을 통해 최선의 결과모델을 선택

  - 만들어진 모델을 외부 연구자와 공유해 학습 재연성 향상

    ```python
    for param_tensor in model.state_dict(): #state_dict: 모델의 파라미터를 표시
        print(param_tensor, "\t", model.state_dict()[param_tensor].size())
        
    torch.save(model.state_dict(),
               os.path.join(MODEL_PATHJ, "model.pt")) #모델의 파라미터를 저장
    
    
    new_model.load =torch.load(os.path.join(MODEL_PATH, "model.pt")) #모델을 불러올 때 사용
    ```

    

- Checkpoints
  - 학습의 중간 결과를 저장하여 최선의 결과를 선택
  - earlystopping 기법 사용시 이전 학습의 결과물을 저장
  - 일반적으로 epoch, loss와 metric 값을 지속적으로 확인 저장
  - colab에서 지속적인 학습을 위해 필요

![check](https://user-images.githubusercontent.com/87477828/130230076-76306a09-5de8-4224-974e-b410eb11f2d5.png)

- Transfer learning

  - 다른 데이터셋으로 만든 모델을 현재 데이터에 적용
  - 일반적으로 대용량 데이터셋으로 만들어진 모델의 성능향상
  - 현재의 딥러닝에서는 가장 일반적인 학습 기법
  - backbone architecture가 잘 학습된 모델에서 일부분만 변경하여 학습을 수행함
  - NLP는 HuggingFace를 백본으로 굉장히 많이 사용한다.

  

- Freezing

  - pretained model을 활용할 시 모델의 일부분을 중지시킴

![freezing](https://user-images.githubusercontent.com/87477828/130231792-ceb020f7-9fd0-4456-8629-02a8613e50f5.png)

```
```

## ch07 Monitoring tools for PyTorch

- Tensorboard

  - TensorFlow의 프로젝트로 만들어진 시각화 도구
  - 학습 그래프, metric, 학습결과의 시각화 지원
  - PyTorch도 연결 가능 -> DL  시각화 핵심 도구

  ```python
  import os
  logs_base_dir = "logs"
  os.makedirs(logs_base_dir, exist_ok=True) #Tensorboard 기록을 위한 directory 생성
  
  from torch.utils.tensorboard import boostcamp #기록 생성 객체 boostcamp 생성
  
  writer = boostcamp(logs_base_dir)
  writer.add_scalar('Loss/train', np.random.random(), n_iter) 
  #add_scalar 함수: scalar 값을 기록
  #Loss/train: loss category에 train 값
  #n_iter : X축의 값
  writer.flush() #값 기록(disk에 쓰기)
  ```

  

- weight&biases

  - 머신러닝 실험을 원할히 지원하기 위한 사용도구

  - 협업, code versioning, 실험 결과 기록 등 제공

  - MLOps의 대표적인 툴로 저변 확대 중

    ```python
    !pip install wandb -q
    
    config = {"epochs": EPOCHS, "batch_size":BATCH_SIZE, "learning_rate": LEARNING_RATE} #config 설정
    wandb.init(project = "my-test-project", config = config)
    #wandb.config.batch_size = BATCH_SIZE
    #wandb.config.learning_rate = LEARNING_RATE
    
    wandb.log({'accuracy':train_acc, 'loss':train_loss}) #기록 add_~~~함수와 동일
    
    ```

    

