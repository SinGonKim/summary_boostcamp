## Lecture1: 최적화 소개 및 강의 개요

### 1. 모델 경량화 강의 개요, 강의 소개

#### 1 경량화

- ML is every where!

  ![a1](https://user-images.githubusercontent.com/87477828/142861083-3c9d1f28-25a0-43e9-899c-83de396dd7b7.png)

  ![a2](https://user-images.githubusercontent.com/87477828/142861256-a06765fc-5c9e-42de-84c6-bd7348a89245.png)

1) On device AI![a3](https://user-images.githubusercontent.com/87477828/142861260-5332a627-c1bf-46f8-b27c-99d42712c858.png)

2)  AI on cloud(or server)

   - 배터리, 저장공간, 연산능력의 제약은 줄어드나, latency와 throughput의 제약이 존재
     - eg. 한 요청의 소요시간, 단위 시간당 처리 가능한 요청수

   - 같은 자원(=돈)으로 더 적은 latency와 더 큰 throughput이 가능하다면?

3. Computation as a key component of AI progress
   - 2012년 이후 큰 AI 모델 학습에 들어가는 연산은 3, 4 개월마다 두배로 증가해옴
   - 2012년 기준, 2018년에는 약 300,000x 증가됨
   - (참고) 연산 측정 방법
     - Counting operations(FLOPs)
     - GPU times

- 경량화는?
  - 모델의 연구와는 별개로, 산업에 적용되기 위해서 거쳐야하는 과정
  - 요구조건(하드웨어 종류, latency 제한, 요구 throughput, 성능)들 간의 trade-off를 고려하여 모델 경량화/최적화를 수행

- 경량화, 최적화의 (대표적인) 종류

  - 네트워크 구조 관점
    - Efficient Architecture Design(+AutoML; Neural Architecture Search(NAS))
    - Network Pruning(찾은 모델 줄이기)
    - Knowledge Distillation
    - Matrix/Tensor Decomposition

  - Hardware 관점

    - Network Quantization
      - fp32로 보통표현 되는데 이를 fp16으로 바꾼다던지 int8 등으로 변환한느 것을 말함

    - Network Compiling

### 2. 강의 목표, 구성

- 일련의 과정을 따라가 보자!; 경량화 컬렉션

  - 경량화, 최적화라는 넓은 분야

  - 특정 주제들은 구체적이고 지엽적

    ![a1](https://user-images.githubusercontent.com/87477828/142866659-1ed54338-169a-47a2-b257-11468206e57b.png)

  - 각 분야마다 약간씩 다르게 적용되는 경량화 variation들(NLP, Speech, Object detection, ...)
  - 이미 가진 모델을 경량화하는 기법들(Pruning, Knowledge distillation)

#### 강의 Overview

1. 강의 개요
2. 대회 소개 및 코드 제출 방법 설명
3. 작은 모델, 좋은 파라미터 찾기: AutoML 이론
4. 작은 모델, 좋은 파라미터 찾기: AutoML 실습
5. 작은 모델, 좋은 파라미터 찾기: Data Augmentation & AutoML 결과 분석
6. 찾은 모델 잘게 찢기 : Tensor Decomposition 이론
7. 찾은 모델 잘게 찢기: Tensor Decomposition 실습
8. 찢은 모델 꾸겨 넣기: Quantization 이론
9. 찢은 모델 꾸겨 넣기: Quantization 실습
10. 학습 파이프라인 최적화 및 마무리