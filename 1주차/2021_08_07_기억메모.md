## 선택과제 문제에 대한 고민

### 선택문제 1번

- 경사하강법의 원리

  - 함수 f(x)가 오목함수(concave) 할 때 접선의 기울기를 이용하여 함수 밑에 접선이 있는 방향으로 가면 함수값이 감소한다는 것을 알 수 있다. 이를 통해 미분값을 빼면 경사하강법(gradient descent)라 하며 함수의 극솟값을 구할 수 있다.

    ![경사하강법 원리](https://user-images.githubusercontent.com/87477828/128550087-0359d2b4-78d9-480f-a78b-2326bb7ded07.png)



![경사하강법 원리1](https://user-images.githubusercontent.com/87477828/128550183-cb82bdff-4d72-4f42-9ed5-48857fb38fde.png)

​                          극값이 0이면 더이상 이동 할 수가 없다. 이론적으로 최적점에 도착한 것이다.

- 경사하강법을 코드로 구현

  - 근데 항상 최적점인 f'(x) = 0 인 지점을 항상 찾을 수 있는 코드를 만들 수 있을까?
    - (X)  극솟값으로 한번에 가는 것이 아니라 계속 이동을 하는 것인데 learning rate에 따라 한번에 극솟값을 찾을 수도 있지만 한참 걸릴 수도 있고(오랜 시간과 좋은 컴퓨터 성능이 필요할 수도 있다.) 오히려 멀어질 수 도 있다(정답을 구할 수 없다.). learning rate의 최적의 값은 알지 못하므로 임의의 작은 수 epsilon 보다 작으면 수렴한다교 표현하는 수학처럼 매우 작은 수 epsilon을 잡아서 극솟값의 근삿값을 구하도록 코드를 짜면 된다.

  - 아이디어를 코드로 구현

    -  위의 원리를 코드로 구현할 수 있도록 짧게 함수를 만들어 보겠다.

      ```python
      def gradient_descent(fun, init_point, lr_rate=1e-2, epsilon=1e-5):
          cnt = 0
          val = init_point
          diff = sym.diff(fun, x)
          func_gradient(fun, init_point) = diff.subs(x, val)
          while np.abs(diff) > epsilon: #abs(f'(x)) < ε 표현
            val = val - lr_rate*diff
            diff = func_gradient(fun, val)
            cnt +=1 #반복횟수 추가 기록
      ```

      