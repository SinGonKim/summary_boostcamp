## Linear Regression

### Gradient Descent를 이용한 Linear Regression을 구현

- Linear Regression에 대해서

  - Linear Regression model에 대한 가정

    - Linear Regression model(선형회귀모형)은 다음과 같이 모수에 대해 선형인 모형이다.

      Y = ωx + b + ε

    - 독립변수 X는 비확률(nonstochastic) 변수

    - 전체 표본에 있어서 독립변수 X는 적어도 서로 다른 두 값을 가진다.(데이터가 1개가 아니다.)

    - E(ε) = 0 이면 E(Y)= ωx + b

    - V(ε) = σ**2 이면 V(Y) =  σ**2

    - Cov(ε1,ε2) = 0 이면 Cov(y1,y2) = 0

- 데이터를 가지고 Linear Regression을 진행하여 모수찾기

  y= w*x + b 를 데이터를 통해 y= x+1 이라는 함수에 근사해지는 경사하강법

  ```python
  #경사하강법, 선형회귀할 때 필수적인 라이브러리
  import numpy as np
  import sympy as sym
  from sympy.abc import x
  from sympy.plotting import plot
  #주어진 데이터 train_x. train_y
  train_x
  train_y[i] = train_x[i] +1
  
  #initialize
  w =0 , b = 0
  lr_rate = α
  n_data = len(train_x)
  errors = [] #e1,e2,.. en
  for i in range(1000): #몇 번 경사하강법을 이용할 것인지 돌리는 반복문
  	pre_y = train_x * w + b #예측값(predict) pre_y
  	# L2 norm과 np_sum 함수 활용해서 error 정의(최소제곱법(Least Square Method,LSM))
  	error = np.sum((pre_y-train_y)**2)/n_data
  	# 위 error를 손실 함수라고 한다. 그러면 gradient를 구하면
  	gradient_w = np.sum(2*(_y - train_y)*train_x)/n_data
  	gradient_b = np.sum(2*(_y - train_y))/n_data
  	# 경사하강법을 통해 변화된 w,b 구하기
  	w -= lr_rate * gradient_w
  	b -= lr_rate * gradient_b
      errors.append(error)
  print("w : {} / b : {} / error : {}".format(w, b, error)) #근사된 모수 w,b 와 error 확인!
  ```

  

